https://blog.csdn.net/zpalyq110/article/details/79527653
https://blog.csdn.net/BF02jgtRS00XKtCx/article/details/82719765
https://github.com/tushushu/imylu/tree/master/imylu   各类算法的实现


训练：
(-0.375-0.175)/2=-0.275
(-0.375+0.275)*(-0.375+0.275)+(-0.175+0.275)*(-0.175+0.275)


(-0.375-0.175+0.225)/3=-0.10833333333
(-0.375+0.10833333333)*(-0.375+0.10833333333) + (-0.175+0.10833333333)*(-0.175+0.10833333333) + (0.225+0.10833333333)*(0.225+0.10833333333)=0.187

所有标签的均值分别减去label是初始的残差，
每棵树根据每个特征进行分裂，计算出均方差最小的分裂点，
均方差的计算是标签与残差的均方误差，有点类似k-means的意思，让left,right的元素能更加各自聚合在最近的距离，所以用均方差。
有了最佳分裂点和第一个残差，这样就可以构建出第一课回归树。

第二棵树的残差如下，是前一轮的残差减去学习率乘以残差，乘以学习率是为了避免过拟合和一步学到位。
0.1*0.3750-0.3750
0.1*0.1750-0.1750
然后接着构建第二棵树，先计算每个特征的最佳分裂点，也是用均方差，计算均方差最小时候的分裂点，
依次构建出每个特征，知道树深度最深或者叶子节点数量满足设置数。

依次构建出M棵树。
GBDT构建完毕。

预测：
f0(x)=所有标签的均值，前面说过了。
f1(x)=样本x输入到第一颗树中，依次搜索到叶子节点，找到的叶子节点的残差值也是预测的标签值，是这棵树预测的值
f2(x)=同上
f3(x)=同上
...
最终预测=强学习器GBDT=F(x)=f0(x)+lr*(f1(x)+f2(x)+...+fm(x))
这里lr是0.1，为什么要乘以0.1呢？